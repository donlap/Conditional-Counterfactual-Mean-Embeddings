{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Conditional Counterfactual Density Estimation Demo\n\nThis notebook demonstrates the use of **Conditional Counterfactual Mean Embedding (CCME)** for estimating conditional distributions of counterfactual outcomes.\n\n## Overview\n\nWe'll use the **IHDP (Infant Health and Development Program)** dataset to:\n1. Estimate conditional counterfactual densities $p(Y^1 | V)$\n2. Compare different estimators (ridge, df, nk)\n3. Evaluate Doubly Robust (dr) vs One-Step (onestep) methods\n4. Visualize heterogeneous treatment effects beyond means"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from omegaconf import OmegaConf\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from ccme import CCDEstimator\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load IHDP Dataset\n",
    "\n",
    "The IHDP dataset contains data from a randomized experiment evaluating the effect of intensive childcare on cognitive test scores.\n",
    "\n",
    "- **Treatment (A)**: Participation in intensive childcare program (0 or 1)\n",
    "- **Outcome (Y)**: Cognitive test score (continuous)\n",
    "- **Covariates (X)**: Mother's age, education, birth weight, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ihdp_data():\n",
    "    \"\"\"\n",
    "    Load IHDP dataset. \n",
    "    For this demo, we'll generate a semi-synthetic version based on IHDP characteristics.\n",
    "    In practice, you can download the real IHDP data from causal benchmarking repositories.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    n = 747  # Approximate IHDP sample size\n",
    "    \n",
    "    # Generate covariates (simplified IHDP structure)\n",
    "    # X1: mother's age (centered), X2: mother's education, X3: birth weight, etc.\n",
    "    X = np.random.randn(n, 10)\n",
    "    \n",
    "    # Treatment assignment (mimicking IHDP's ~2:1 control:treatment ratio)\n",
    "    propensity = 1 / (1 + np.exp(-0.5 * (X[:, 0] + X[:, 1])))\n",
    "    A = np.random.binomial(1, propensity)\n",
    "    \n",
    "    # Potential outcomes with heterogeneous effects\n",
    "    Y0 = 80 + 10 * X[:, 0] + 5 * X[:, 1] - 3 * X[:, 2] + np.random.randn(n) * 5\n",
    "    treatment_effect = 10 + 5 * X[:, 0] - 2 * X[:, 1]  # Heterogeneous\n",
    "    Y1 = Y0 + treatment_effect + np.random.randn(n) * 5\n",
    "    \n",
    "    # Observed outcome\n",
    "    Y = A * Y1 + (1 - A) * Y0\n",
    "    \n",
    "    return X, Y[:, None], A, Y0, Y1\n",
    "\n",
    "# Load data\n",
    "X, Y, A, Y0_true, Y1_true = load_ihdp_data()\n",
    "n_samples = len(Y)\n",
    "\n",
    "print(f\"Dataset loaded: {n_samples} samples\")\n",
    "print(f\"  - Treated: {A.sum()} ({A.mean()*100:.1f}%)\")\n",
    "print(f\"  - Control: {(1-A).sum()} ({(1-A.mean())*100:.1f}%)\")\n",
    "print(f\"  - Covariates: {X.shape[1]} dimensions\")\n",
    "print(f\"  - Outcome range: [{Y.min():.1f}, {Y.max():.1f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Outcome distribution by treatment\n",
    "axes[0].hist(Y[A==0], bins=30, alpha=0.6, label='Control (A=0)', density=True)\n",
    "axes[0].hist(Y[A==1], bins=30, alpha=0.6, label='Treated (A=1)', density=True)\n",
    "axes[0].set_xlabel('Cognitive Test Score (Y)')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].set_title('Observed Outcome Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: True treatment effects (heterogeneity)\n",
    "true_te = Y1_true - Y0_true\n",
    "axes[1].scatter(X[:, 0], true_te, alpha=0.4, s=20)\n",
    "axes[1].axhline(y=true_te.mean(), color='r', linestyle='--', label=f'ATE = {true_te.mean():.2f}')\n",
    "axes[1].set_xlabel('Covariate Xâ‚ (Mother\\'s Age)')\n",
    "axes[1].set_ylabel('True Treatment Effect (YÂ¹ - Yâ°)')\n",
    "axes[1].set_title('Heterogeneous Treatment Effects')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAverage Treatment Effect (ATE): {true_te.mean():.2f}\")\n",
    "print(f\"Treatment Effect Range: [{true_te.min():.2f}, {true_te.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Interactive Configuration\n\nUse the form below to configure the estimator. You can adjust:\n- **Method**: ridge (kernel), df (deep features), or nk (neural-kernel)\n- **Mode**: dr (doubly robust), onestep, ipw, or pi (plug-in)\n- **Hyperparameters**: Model architecture and training settings"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create interactive configuration widgets\n\n# Method and Mode selection\nmethod_widget = widgets.Dropdown(\n    options=['ridge', 'df', 'nk'],\n    value='df',\n    description='Method:',\n    style={'description_width': '150px'}\n)\n\nmode_widget = widgets.Dropdown(\n    options=['dr', 'onestep', 'ipw', 'pi'],\n    value='dr',\n    description='Mode:',\n    style={'description_width': '150px'}\n)\n\n# Model hyperparameters\noutput_dim_widget = widgets.IntSlider(\n    value=20,\n    min=5,\n    max=100,\n    step=5,\n    description='Number of Atoms/Features:',\n    style={'description_width': '150px'}\n)\n\nhidden_dim_widget = widgets.Text(\n    value='[20, 20]',\n    description='Hidden Layers:',\n    style={'description_width': '150px'},\n    placeholder='[20, 20]'\n)\n\nsigma_widget = widgets.FloatSlider(\n    value=2.0,\n    min=0.1,\n    max=10.0,\n    step=0.1,\n    description='Kernel Bandwidth (Ïƒ):',\n    style={'description_width': '150px'}\n)\n\nlambda_widget = widgets.FloatText(\n    value=20.0,\n    description='Regularization (Î»):',\n    style={'description_width': '150px'}\n)\n\n# Training parameters\nlr_or_widget = widgets.FloatText(\n    value=2e-2,\n    description='Learning Rate (Stage 1):',\n    style={'description_width': '150px'}\n)\n\nlr_fi_widget = widgets.FloatText(\n    value=2e-2,\n    description='Learning Rate (Stage 2):',\n    style={'description_width': '150px'}\n)\n\nepoch_or_widget = widgets.IntSlider(\n    value=1000,\n    min=100,\n    max=10000,\n    step=100,\n    description='Epochs (Stage 1):',\n    style={'description_width': '150px'}\n)\n\nepoch_fi_widget = widgets.IntSlider(\n    value=500,\n    min=100,\n    max=5000,\n    step=100,\n    description='Epochs (Stage 2):',\n    style={'description_width': '150px'}\n)\n\nbatch_size_widget = widgets.IntSlider(\n    value=128,\n    min=32,\n    max=512,\n    step=32,\n    description='Batch Size:',\n    style={'description_width': '150px'}\n)\n\n# Validation and early stopping\nvalid_size_widget = widgets.FloatSlider(\n    value=0.15,\n    min=0.0,\n    max=0.5,\n    step=0.05,\n    description='Validation Fraction:',\n    style={'description_width': '150px'}\n)\n\npatience_widget = widgets.IntSlider(\n    value=50,\n    min=0,\n    max=200,\n    step=10,\n    description='Early Stop Patience:',\n    style={'description_width': '150px'}\n)\n\nseed_widget = widgets.IntText(\n    value=42,\n    description='Random Seed:',\n    style={'description_width': '150px'}\n)\n\nverbose_widget = widgets.Checkbox(\n    value=True,\n    description='Verbose Training',\n    style={'description_width': '150px'}\n)\n\nn_folds_widget = widgets.IntText(\n    value=0,\n    description='Cross-Fit Folds (0=off):',\n    style={'description_width': '150px'}\n)\n\n# Organize widgets into sections\nprint(\"\\n\" + \"=\"*60)\nprint(\"CONFIGURATION FORM\")\nprint(\"=\"*60)\n\nprint(\"\\nðŸ“Š METHOD & MODE\")\ndisplay(method_widget, mode_widget)\n\nprint(\"\\nðŸ”§ MODEL HYPERPARAMETERS\")\ndisplay(output_dim_widget, hidden_dim_widget, sigma_widget, lambda_widget)\n\nprint(\"\\nðŸŽ“ TRAINING PARAMETERS\")\ndisplay(lr_or_widget, lr_fi_widget, epoch_or_widget, epoch_fi_widget, batch_size_widget)\n\nprint(\"\\nâœ… VALIDATION & EARLY STOPPING\")\ndisplay(valid_size_widget, patience_widget)\n\nprint(\"\\nâš™ï¸ OTHER SETTINGS\")\ndisplay(seed_widget, verbose_widget, n_folds_widget)\n\nprint(\"\\n\" + \"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Configuration and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build configuration from widgets\ndef build_config():\n    \"\"\"Build OmegaConf configuration from widget values\"\"\"\n    \n    # Parse hidden dimensions\n    try:\n        hidden_dim = eval(hidden_dim_widget.value)\n        if not isinstance(hidden_dim, list):\n            hidden_dim = [hidden_dim]\n    except:\n        hidden_dim = [20, 20]\n        print(\"âš ï¸ Invalid hidden_dim format, using default [20, 20]\")\n    \n    # Determine lambda value (null for nk, float for df/ridge)\n    lamb_value = None if method_widget.value == 'nk' else lambda_widget.value\n    \n    # Validation size (0 means no validation)\n    valid_size = valid_size_widget.value if valid_size_widget.value > 0 else None\n    \n    # Patience (0 means no early stopping)\n    patience = patience_widget.value if patience_widget.value > 0 else None\n    \n    # Cross-fitting folds (0 means off / 50-50 split)\n    n_folds = n_folds_widget.value if n_folds_widget.value >= 2 else None\n    \n    config_dict = {\n        'method': method_widget.value,\n        'mode': mode_widget.value,\n        'seed': seed_widget.value,\n        'verbose': verbose_widget.value,\n        'n_folds': n_folds,\n        'model': {\n            'output_dim': output_dim_widget.value,\n            'hidden_dim': hidden_dim,\n            'sigma_init': sigma_widget.value,\n            'lamb': lamb_value,\n            'learn_sigma': False\n        },\n        'propensity_score_model': {\n            '_target_': 'sklearn.ensemble.RandomForestClassifier',\n            'max_depth': 4\n        },\n        'train': {\n            'lr_or': lr_or_widget.value,\n            'lr_fi': lr_fi_widget.value,\n            'batch_size_or': batch_size_widget.value,\n            'batch_size_fi': batch_size_widget.value,\n            'epoch_or': epoch_or_widget.value,\n            'epoch_fi': epoch_fi_widget.value,\n            'valid_size_or': valid_size,\n            'valid_size_fi': valid_size,\n            'patience_or': patience,\n            'patience_fi': patience\n        },\n        'test': {\n            'num_bin': 1000\n        }\n    }\n    \n    return OmegaConf.create(config_dict)\n\n# Build config\ncfg = build_config()\n\nprint(\"\\nâœ“ Configuration built successfully!\")\nprint(f\"\\nMethod: {cfg.method} | Mode: {cfg.mode}\")\nprint(f\"Model: {cfg.model.output_dim} atoms/features, Ïƒ={cfg.model.sigma_init}, Î»={cfg.model.lamb}\")\nprint(f\"Training: {cfg.train.epoch_or} epochs (stage 1), {cfg.train.epoch_fi} epochs (stage 2)\")\nif cfg.train.valid_size_or:\n    print(f\"Validation: {cfg.train.valid_size_or*100:.0f}% with patience={cfg.train.patience_or}\")\nif cfg.n_folds:\n    print(f\"Cross-fitting: {cfg.n_folds} folds\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING CCME ESTIMATOR\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Use first 5 covariates for conditioning\n",
    "V = X[:, :5]\n",
    "\n",
    "# Initialize and fit\n",
    "estimator = CCDEstimator(cfg)\n",
    "estimator.fit(X=X, V=V, Y=Y, A=A)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ“ TRAINING COMPLETED\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Estimated Densities\n",
    "\n",
    "We'll visualize the estimated conditional counterfactual densities for different covariate profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select test points (different covariate profiles)\n",
    "# Profile 1: Low mother's age/education\n",
    "# Profile 2: High mother's age/education\n",
    "\n",
    "V_test = np.array([\n",
    "    [-1.5, -1.0, 0.0, 0.0, 0.0],  # Profile 1: Low\n",
    "    [1.5, 1.0, 0.0, 0.0, 0.0]      # Profile 2: High\n",
    "])\n",
    "\n",
    "# Define grid for density estimation\n",
    "y_min, y_max = Y.min() - 10, Y.max() + 10\n",
    "Y_grid = np.linspace(y_min, y_max, 500)[:, None]\n",
    "\n",
    "# Predict densities\n",
    "pdf_est = estimator.predict(V_eval=V_test, Y_grid=Y_grid)\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for i, (ax, profile_name) in enumerate(zip(axes, ['Low Covariate Profile', 'High Covariate Profile'])):\n",
    "    ax.plot(Y_grid, pdf_est[i], linewidth=2.5, label=f'Estimated p(YÂ¹|V)', color='C0')\n",
    "    ax.fill_between(Y_grid.flatten(), 0, pdf_est[i], alpha=0.3, color='C0')\n",
    "    \n",
    "    # Add vertical lines for mean and quantiles\n",
    "    mean_est = np.sum(Y_grid.flatten() * pdf_est[i]) * (Y_grid[1] - Y_grid[0])\n",
    "    ax.axvline(mean_est, color='red', linestyle='--', linewidth=2, label=f'Mean = {mean_est:.1f}')\n",
    "    \n",
    "    ax.set_xlabel('Outcome YÂ¹ (Cognitive Test Score)', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.set_title(f'{profile_name}\\n{cfg.method}-{cfg.mode}', fontsize=13, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Estimated Conditional Means:\")\n",
    "for i, profile in enumerate(['Low', 'High']):\n",
    "    mean_est = np.sum(Y_grid.flatten() * pdf_est[i]) * (Y_grid[1] - Y_grid[0])\n",
    "    print(f\"  {profile} profile: E[YÂ¹|V] = {mean_est:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional Insights Beyond Means\n",
    "\n",
    "Extract richer information from the estimated densities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distributional_stats(y_grid, pdf):\n",
    "    \"\"\"Compute mean, variance, quantiles from density\"\"\"\n",
    "    dy = y_grid[1] - y_grid[0]\n",
    "    \n",
    "    # Mean\n",
    "    mean = np.sum(y_grid * pdf) * dy\n",
    "    \n",
    "    # Variance\n",
    "    variance = np.sum((y_grid - mean)**2 * pdf) * dy\n",
    "    std = np.sqrt(variance)\n",
    "    \n",
    "    # Quantiles\n",
    "    cdf = np.cumsum(pdf) * dy\n",
    "    q25 = y_grid[np.argmin(np.abs(cdf - 0.25))]\n",
    "    q50 = y_grid[np.argmin(np.abs(cdf - 0.50))]\n",
    "    q75 = y_grid[np.argmin(np.abs(cdf - 0.75))]\n",
    "    \n",
    "    return {\n",
    "        'mean': mean,\n",
    "        'std': std,\n",
    "        'q25': q25,\n",
    "        'q50': q50,\n",
    "        'q75': q75\n",
    "    }\n",
    "\n",
    "# Compute stats for both profiles\n",
    "y_grid_flat = Y_grid.flatten()\n",
    "\n",
    "stats_low = compute_distributional_stats(y_grid_flat, pdf_est[0])\n",
    "stats_high = compute_distributional_stats(y_grid_flat, pdf_est[1])\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame({\n",
    "    'Low Profile': [f\"{stats_low['mean']:.2f}\", f\"{stats_low['std']:.2f}\", \n",
    "                    f\"{stats_low['q25']:.2f}\", f\"{stats_low['q50']:.2f}\", f\"{stats_low['q75']:.2f}\"],\n",
    "    'High Profile': [f\"{stats_high['mean']:.2f}\", f\"{stats_high['std']:.2f}\",\n",
    "                     f\"{stats_high['q25']:.2f}\", f\"{stats_high['q50']:.2f}\", f\"{stats_high['q75']:.2f}\"]\n",
    "}, index=['Mean', 'Std Dev', '25th Percentile', 'Median', '75th Percentile'])\n",
    "\n",
    "print(\"\\nðŸ“Š DISTRIBUTIONAL STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(results_df)\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Interpretation\n",
    "mean_diff = stats_high['mean'] - stats_low['mean']\n",
    "print(f\"\\nðŸ’¡ INSIGHTS:\")\n",
    "print(f\"  â€¢ Conditional Average Treatment Effect difference: {mean_diff:.2f}\")\n",
    "print(f\"  â€¢ High profile has {'higher' if mean_diff > 0 else 'lower'} expected outcomes\")\n",
    "print(f\"  â€¢ Low profile uncertainty (std): {stats_low['std']:.2f}\")\n",
    "print(f\"  â€¢ High profile uncertainty (std): {stats_high['std']:.2f}\")\n",
    "print(f\"  â€¢ IQR (Low): {stats_low['q75'] - stats_low['q25']:.2f}\")\n",
    "print(f\"  â€¢ IQR (High): {stats_high['q75'] - stats_high['q25']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Multiple Methods (Optional)\n",
    "\n",
    "Compare CME, DF, and NN estimators on the same profile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train all three methods\nmethods = ['ridge', 'df', 'nk']\ncolors = ['C0', 'C1', 'C2']\nestimators = {}\npredictions = {}\n\nprint(\"\\nTraining multiple methods for comparison...\\n\")\n\nfor method in methods:\n    print(f\"Training {method}...\")\n    \n    # Update config\n    cfg_temp = OmegaConf.create(OmegaConf.to_container(cfg))\n    cfg_temp.method = method\n    \n    # Adjust hyperparameters by method\n    if method == 'ridge':\n        cfg_temp.model.lamb = 20.0\n        cfg_temp.train.epoch_or = 1\n        cfg_temp.train.epoch_fi = 1\n    elif method == 'df':\n        cfg_temp.model.lamb = 20.0\n        cfg_temp.train.epoch_or = 1000\n        cfg_temp.train.epoch_fi = 500\n    else:  # nk\n        cfg_temp.model.lamb = None\n        cfg_temp.train.epoch_or = 1000\n        cfg_temp.train.epoch_fi = 500\n    \n    cfg_temp.verbose = False  # Suppress output for cleaner comparison\n    \n    # Train\n    est = CCDEstimator(cfg_temp)\n    est.fit(X=X, V=V, Y=Y, A=A)\n    \n    # Predict on first profile\n    pred = est.predict(V_eval=V_test[:1], Y_grid=Y_grid)\n    \n    estimators[method] = est\n    predictions[method] = pred[0]\n\nprint(\"\\nâœ“ All methods trained!\\n\")\n\n# Plot comparison\nplt.figure(figsize=(10, 6))\n\nfor method, color in zip(methods, colors):\n    plt.plot(Y_grid, predictions[method], linewidth=2.5, label=f'{method}-{cfg.mode}', color=color, alpha=0.8)\n\nplt.xlabel('Outcome YÂ¹ (Cognitive Test Score)', fontsize=12)\nplt.ylabel('Density', fontsize=12)\nplt.title(f'Method Comparison: Estimated p(YÂ¹|V)\\nLow Covariate Profile', fontsize=13, fontweight='bold')\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nðŸ“Š Estimated Means by Method:\")\nfor method in methods:\n    mean_est = np.sum(y_grid_flat * predictions[method]) * (Y_grid[1] - Y_grid[0])\n    print(f\"  {method}: E[YÂ¹|V] = {mean_est:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook demonstrated:\n\n1. âœ… **Interactive Configuration**: Use widgets instead of YAML files\n2. âœ… **Conditional Density Estimation**: Estimate $p(Y^1|V)$ for different covariate profiles\n3. âœ… **Distributional Insights**: Extract means, variances, quantiles beyond ATEs\n4. âœ… **Method Comparison**: Compare ridge, df, and nk estimators\n5. âœ… **Heterogeneous Effects**: Visualize how treatment effects vary across populations\n\n### Next Steps:\n\n- Try different methods (ridge, df, nk) and modes (dr, onestep)\n- Adjust hyperparameters to see their effects\n- Use real IHDP data for more realistic results\n- Explore other datasets (LaLonde, 401k, etc.)\n\n### Key Takeaways:\n\n- **Beyond means**: Full densities reveal heterogeneity hidden by average effects\n- **Double robustness**: dr mode provides protection against model misspecification\n- **Flexible estimation**: Neural methods (df, nk) scale to complex settings"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}